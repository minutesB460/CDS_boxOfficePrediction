{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\remote\n",
      "[nltk_data]     desktop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental arrows\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols & pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Symbols & pictographs extended-A\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols & pictographs extended-B\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# Remove stopwords，punctuations，html tags，emojis and transform into lowercase\n",
    "def preprocess_text(text):\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\",\", \" , \").replace(\".\", \" . \").replace(\"-\", \" - \").replace(\"/\", \" / \")\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    words = text.split()\n",
    "    filtered_sentence = \" \".join(word.translate(table) for word in words if word not in stop_words)\n",
    "    return filtered_sentence.strip()\n",
    "\n",
    "def scrape_imdb_reviews(movie_id):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    url = f\"https://www.imdb.com/title/{movie_id}/reviews\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    load_more_clicked = 0\n",
    "    # Click the btn to load more reviews\n",
    "    while load_more_clicked < 5:\n",
    "        try:\n",
    "            load_more_button = driver.find_element(By.CSS_SELECTOR, \"button.ipc-see-more__button\")\n",
    "            # load_more_button.click()\n",
    "            load_more_button.send_keys('\\n')\n",
    "            wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, \".ipl-load-more__load-indicator\")))\n",
    "            time.sleep(2)\n",
    "            load_more_clicked +=1\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            break\n",
    "    print(load_more_clicked)\n",
    "\n",
    "    # Get the full HTML \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()  \n",
    "\n",
    "    #Extract movie name\n",
    "    movie_name = soup.select_one(\"[data-testid='subtitle']\")\n",
    "\n",
    "    # Extract review data\n",
    "    title_reviews = []\n",
    "    content_reviews = []\n",
    "    review_date = []\n",
    "\n",
    "    for review_block in soup.select(\"article.user-review-item\"):\n",
    "        try:\n",
    "            title = review_block.select_one(\"h3.ipc-title__text\").text.strip()\n",
    "            content = review_block.select_one(\"div.ipc-html-content-inner-div\").text.strip()\n",
    "            date = review_block.select_one(\"li.review-date\").text.strip()\n",
    "            title_reviews.append(title)\n",
    "            content_reviews.append(content)\n",
    "            review_date.append(date)\n",
    "        except AttributeError:\n",
    "            print(\"Skipping a re view that cannot be parsed\")\n",
    "\n",
    "    # check the number of extracted reviews\n",
    "    print(f\"number of review titles: {len(title_reviews)}\")\n",
    "    print(f\"number of review content: {len(content_reviews)}\")\n",
    "    print(f\"number of review date: {len(review_date)}\")\n",
    "\n",
    "    processed_titles = [preprocess_text(title) for title in title_reviews]\n",
    "    processed_reviews = [preprocess_text(review) for review in content_reviews]\n",
    "\n",
    "    reviews_df = pd.DataFrame({\n",
    "        \"Movie Name\": movie_name,\n",
    "        \"Review Title\": processed_titles,\n",
    "        \"Review Content\": processed_reviews,\n",
    "        \"Review Date\": review_date\n",
    "    })\n",
    "\n",
    "    csv_filename = f\"{movie_id}_Cleaned_Reviews.csv\"\n",
    "    reviews_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(reviews_df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 112\n",
      "number of review content: 112\n",
      "number of review date: 112\n",
      "                  Movie Name  \\\n",
      "0  Spider-Man: Far from Home   \n",
      "1  Spider-Man: Far from Home   \n",
      "2  Spider-Man: Far from Home   \n",
      "3  Spider-Man: Far from Home   \n",
      "4  Spider-Man: Far from Home   \n",
      "\n",
      "                                        Review Title  \\\n",
      "0                                   teen movie twist   \n",
      "1                         one best spider  man movie   \n",
      "2                                            amazing   \n",
      "3                                   effort execution   \n",
      "4  perfectly uses comic book material modernising...   \n",
      "\n",
      "                                      Review Content   Review Date  \n",
      "0  movie breath fresh air  fun romp europe favour...   Sep 7, 2019  \n",
      "1  amazing movie  many surprises movie  3d effect...  Jul 15, 2019  \n",
      "2  movie much better 1st installment  cgi good gy...  Sep 11, 2019  \n",
      "3  movie hands best mcu movie yet  beginning end ...   Jul 3, 2019  \n",
      "4  perfect spiderman film  perfect mysterio  perf...  Jul 13, 2019  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remote desktop\\AppData\\Local\\Temp\\ipykernel_47272\\3463117793.py:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Access the IMDB review page\n",
    "movie_id = \"tt6320628\" \n",
    "scrape_imdb_reviews(movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Function to get IMDb Top 250 Movie IDs\n",
    "def get_imdb_top_250():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in the background\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    url = \"https://www.imdb.com/chart/top/\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Allow time for the page to load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    movies = []\n",
    "    # for movie in soup.select(\"div.sc-ee514ad1-0.kYZRWL.cli-poster-container\"):\n",
    "    #     movie_title = movie.select_one(\"h3.ipc-title__text\")\n",
    "    #     movie_id = movie[\"href\"].split(\"/\")[2]  # Extracts the movie ID (e.g., tt0111161)\n",
    "    #     movies.append({\"movie_id\": movie_id, \"title\": movie_title})\n",
    "\n",
    "    for movie in soup.select(\"h3.ipc-title__text\"):  \n",
    "        movie_title = movie.text.strip()\n",
    "        movies.append({\"Movie Title\": movie_title})\n",
    "\n",
    "    return movies[:5]  # Return top 5 movies (change this if needed)\n",
    "\n",
    "get_imdb_top_250()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "\n",
    "res = requests.get(\"https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm.I\")\n",
    "#print(res)\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "for card in soup.select('li.ipc-metadata-list-summary-item'):\n",
    "    data.append({\n",
    "        \"title\": card.select_one('h3.ipc-title__text').text.strip()\n",
    "        # \"year\": card.select_one('.titleColumn span').text,\n",
    "        # 'rating': card.select_one('td[class=\"ratingColumn imdbRating\"]').get_text(strip=True)\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "#df.to_csv('out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Idiots : 1187043\n",
      "3 Idiots : 3685624\n",
      "3 Idiots : 28238283\n",
      "3 Idiots w/GUNS : 0222441\n",
      "3 Idiots and a Wise Man : 21612358\n",
      "Three Idiots to a Team : 29720863\n",
      "Mugguru Monagallu : 15121916\n",
      "3 Idiots on Wheels : 6689378\n",
      "Scotch Mist - A Tale of Three English Idiots in Search of Britain's Northernmost Monsters : 31444403\n",
      "Kidnap in Rome : 1575673\n",
      "3 Idiots : 12049418\n",
      "3 Idiots : 33501685\n",
      "3 Idiot Heroes : 30247415\n",
      "Three Idiots : 16345748\n",
      "Three Idiots : 34207697\n",
      "The Idiots : 0154421\n",
      "3 Idiots Try Candy! : 8474256\n",
      "God and 3 Idiots : 25393152\n",
      "Confessions of Three Idiots : 21124554\n",
      "Idiots Are People Three! : 2179303\n"
     ]
    }
   ],
   "source": [
    "# importing the module \n",
    "import imdb \n",
    "   \n",
    "# creating instance of IMDb \n",
    "ia = imdb.IMDb() \n",
    "   \n",
    "# name  \n",
    "name = \"3 idiots\"\n",
    "   \n",
    "# searching the name  \n",
    "search = ia.search_movie(name) \n",
    "  \n",
    "  \n",
    "# loop for printing the name and id \n",
    "for i in range(len(search)): \n",
    "      \n",
    "    # getting the id \n",
    "    id = search[i].movieID \n",
    "      \n",
    "    # printing it \n",
    "    print(search[i]['title'] + \" : \" + id ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     filtered_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(word\u001b[38;5;241m.\u001b[39mtranslate(table) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_sentence\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 28\u001b[0m processed_titles \u001b[38;5;241m=\u001b[39m [preprocess_text(title) \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtitle_reviews\u001b[49m]\n\u001b[0;32m     29\u001b[0m processed_reviews \u001b[38;5;241m=\u001b[39m [preprocess_text(review) \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m content_reviews]\n\u001b[0;32m     31\u001b[0m reviews_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie Name\u001b[39m\u001b[38;5;124m\"\u001b[39m: movie_name,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview Title\u001b[39m\u001b[38;5;124m\"\u001b[39m: processed_titles,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview Content\u001b[39m\u001b[38;5;124m\"\u001b[39m: processed_reviews,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview Date\u001b[39m\u001b[38;5;124m\"\u001b[39m: review_date\n\u001b[0;32m     36\u001b[0m })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'title_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental arrows\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols & pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Symbols & pictographs extended-A\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols & pictographs extended-B\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# Remove stopwords，punctuations，html tags，emojis and transform into lowercase\n",
    "def preprocess_text(text):\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\",\", \" , \").replace(\".\", \" . \").replace(\"-\", \" - \").replace(\"/\", \" / \")\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    words = text.split()\n",
    "    filtered_sentence = \" \".join(word.translate(table) for word in words if word not in stop_words)\n",
    "    return filtered_sentence.strip()\n",
    "\n",
    "processed_titles = [preprocess_text(title) for title in title_reviews]\n",
    "processed_reviews = [preprocess_text(review) for review in content_reviews]\n",
    "\n",
    "reviews_df = pd.DataFrame({\n",
    "    \"Movie Name\": movie_name,\n",
    "    \"Review Title\": processed_titles,\n",
    "    \"Review Content\": processed_reviews,\n",
    "    \"Review Date\": review_date\n",
    "})\n",
    "\n",
    "csv_filename = f\"{movie_id}_Cleaned_Reviews.csv\"\n",
    "reviews_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(reviews_df.head())  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
