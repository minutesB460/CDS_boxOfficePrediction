{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\remote\n",
      "[nltk_data]     desktop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental arrows\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols & pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Symbols & pictographs extended-A\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols & pictographs extended-B\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# Remove stopwords，punctuations，html tags，emojis and transform into lowercase\n",
    "def preprocess_text(text):\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\",\", \" , \").replace(\".\", \" . \").replace(\"-\", \" - \").replace(\"/\", \" / \")\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    words = text.split()\n",
    "    filtered_sentence = \" \".join(word.translate(table) for word in words if word not in stop_words)\n",
    "    return filtered_sentence.strip()\n",
    "\n",
    "def scrape_imdb_reviews(movie_id):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    url = f\"https://www.imdb.com/title/{movie_id}/reviews\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    load_more_clicked = 0\n",
    "    # Click the btn to load more reviews\n",
    "    while load_more_clicked < 1:\n",
    "        try:\n",
    "            load_more_button = driver.find_element(By.CSS_SELECTOR, \"button.ipc-see-more__button\")\n",
    "            # load_more_button.click()\n",
    "            load_more_button.send_keys('\\n')\n",
    "            wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, \".ipl-load-more__load-indicator\")))\n",
    "            time.sleep(2)\n",
    "            load_more_clicked +=1\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            break\n",
    "    print(load_more_clicked)\n",
    "\n",
    "    # Get the full HTML \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()  \n",
    "\n",
    "    # #Extract movie name\n",
    "    # movie_name = soup.select_one(\"[data-testid='subtitle']\")\n",
    "\n",
    "    # Extract review data\n",
    "    title_reviews = []\n",
    "    content_reviews = []\n",
    "    review_date = []\n",
    "\n",
    "    for review_block in soup.select(\"article.user-review-item\"):\n",
    "        try:\n",
    "            title = review_block.select_one(\"h3.ipc-title__text\").text.strip()\n",
    "            content = review_block.select_one(\"div.ipc-html-content-inner-div\").text.strip()\n",
    "            date = review_block.select_one(\"li.review-date\").text.strip()\n",
    "            title_reviews.append(title)\n",
    "            content_reviews.append(content)\n",
    "            review_date.append(date)\n",
    "        except AttributeError:\n",
    "            print(\"Skipping a re view that cannot be parsed\")\n",
    "\n",
    "    # check the number of extracted reviews\n",
    "    print(f\"number of review titles: {len(title_reviews)}\")\n",
    "    print(f\"number of review content: {len(content_reviews)}\")\n",
    "    print(f\"number of review date: {len(review_date)}\")\n",
    "\n",
    "    processed_titles = [preprocess_text(title) for title in title_reviews]\n",
    "    processed_reviews = [preprocess_text(review) for review in content_reviews]\n",
    "\n",
    "    reviews_df = pd.DataFrame({\n",
    "        \"Movie ID\": movie_id,\n",
    "        \"Review Title\": processed_titles,\n",
    "        \"Review Content\": processed_reviews,\n",
    "        \"Review Date\": review_date\n",
    "    })\n",
    "\n",
    "    # csv_filename = f\"{movie_id}_Cleaned_Reviews.csv\"\n",
    "    # reviews_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 23\n",
      "number of review content: 23\n",
      "number of review date: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remote desktop\\AppData\\Local\\Temp\\ipykernel_16348\\434011934.py:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1 movies.\n",
      "0\n",
      "number of review titles: 0\n",
      "number of review content: 0\n",
      "number of review date: 0\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 8\n",
      "number of review content: 8\n",
      "number of review date: 8\n",
      "Scraped 2 movies.\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 14\n",
      "number of review content: 14\n",
      "number of review date: 14\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 33\n",
      "number of review content: 33\n",
      "number of review date: 33\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 35\n",
      "number of review content: 35\n",
      "number of review date: 35\n",
      "Scraped 3 movies.\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 3\n",
      "number of review content: 3\n",
      "number of review date: 3\n",
      "0\n",
      "number of review titles: 0\n",
      "number of review content: 0\n",
      "number of review date: 0\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 47\n",
      "number of review content: 47\n",
      "number of review date: 47\n",
      "Scraped 4 movies.\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 26\n",
      "number of review content: 26\n",
      "number of review date: 26\n",
      "Scraped 5 movies.\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 38\n",
      "number of review content: 38\n",
      "number of review date: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remote desktop\\AppData\\Local\\Temp\\ipykernel_16348\\434011934.py:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 6 movies.\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 23\n",
      "number of review content: 23\n",
      "number of review date: 23\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 2\n",
      "number of review content: 2\n",
      "number of review date: 2\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 17\n",
      "number of review content: 17\n",
      "number of review date: 17\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 18\n",
      "number of review content: 18\n",
      "number of review date: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remote desktop\\AppData\\Local\\Temp\\ipykernel_16348\\434011934.py:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 2\n",
      "number of review content: 2\n",
      "number of review date: 2\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 6\n",
      "number of review content: 6\n",
      "number of review date: 6\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 12\n",
      "number of review content: 12\n",
      "number of review date: 12\n",
      "Scraped 7 movies.\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 48\n",
      "number of review content: 48\n",
      "number of review date: 48\n",
      "0\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 1\n",
      "number of review content: 1\n",
      "number of review date: 1\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 46\n",
      "number of review content: 46\n",
      "number of review date: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remote desktop\\AppData\\Local\\Temp\\ipykernel_16348\\434011934.py:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 8 movies.\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 31\n",
      "number of review content: 31\n",
      "number of review date: 31\n",
      "Scraped 9 movies.\n",
      "0\n",
      "number of review titles: 11\n",
      "number of review content: 11\n",
      "number of review date: 11\n",
      "Scraped 10 movies.\n",
      "1\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 23\n",
      "number of review content: 23\n",
      "number of review date: 23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m random_movie_id \u001b[38;5;241m=\u001b[39m get_random_movie_id(min_votes\u001b[38;5;241m=\u001b[39mmin_votes)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Scrape IMDb reviews for the movie.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m reviews_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_imdb_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_movie_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# If reviews_df is empty, skip this movie and continue.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reviews_df\u001b[38;5;241m.\u001b[39mempty:\n",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mscrape_imdb_reviews\u001b[1;34m(movie_id)\u001b[0m\n\u001b[0;32m     48\u001b[0m     load_more_button\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     49\u001b[0m     wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39minvisibility_of_element_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ipl-load-more__load-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     load_more_clicked \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# print(e)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from utils import get_random_movie_id, scrape_worldwide_box_office\n",
    "\n",
    "scraped = 0\n",
    "to_scrape = 100\n",
    "min_votes = 1000  # Only select movies with at least 1000 votes\n",
    "\n",
    "# Create an empty DataFrame to store all movie reviews\n",
    "all_reviews_df = pd.DataFrame()\n",
    "\n",
    "while scraped < to_scrape:\n",
    "    # Get a random movie ID that meets the minimum votes criteria.\n",
    "    random_movie_id = get_random_movie_id(min_votes=min_votes)\n",
    "    \n",
    "    # Scrape IMDb reviews for the movie.\n",
    "    reviews_df = scrape_imdb_reviews(random_movie_id)\n",
    "    \n",
    "    # If reviews_df is empty, skip this movie and continue.\n",
    "    if reviews_df.empty:\n",
    "        continue\n",
    "\n",
    "    # Try to get the worldwide box office figure; if it fails, skip to the next movie.\n",
    "    try:\n",
    "        box_office_str = scrape_worldwide_box_office(random_movie_id)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    # Extract the number from the box office string, e.g., \"$47,680,966\" becomes 47680966.\n",
    "    match = re.search(r'\\$([\\d,]+)', box_office_str)\n",
    "    if match:\n",
    "        box_office_value = int(match.group(1).replace(',', ''))\n",
    "    else:\n",
    "        continue  # If no number can be extracted, skip this movie.\n",
    "\n",
    "    # Add the worldwide box office value as a new column to the reviews DataFrame.\n",
    "    reviews_df['Worldwide BO'] = box_office_value\n",
    "\n",
    "    # Append the movie's reviews to the cumulative DataFrame.\n",
    "    all_reviews_df = pd.concat([all_reviews_df, reviews_df], ignore_index=True)\n",
    "\n",
    "    scraped += 1\n",
    "    print(f\"Scraped {scraped} movies.\")\n",
    "\n",
    "# Optionally, save the cumulative DataFrame to a CSV file.\n",
    "all_reviews_df.to_csv(\"movie_reviews_box_office.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "Skipping a re view that cannot be parsed\n",
      "number of review titles: 111\n",
      "number of review content: 111\n",
      "number of review date: 111\n",
      "                  Movie Name                Review Title  \\\n",
      "0  Spider-Man: Far from Home            teen movie twist   \n",
      "1  Spider-Man: Far from Home  one best spider  man movie   \n",
      "2  Spider-Man: Far from Home                     amazing   \n",
      "3  Spider-Man: Far from Home            effort execution   \n",
      "4  Spider-Man: Far from Home                      adored   \n",
      "\n",
      "                                      Review Content   Review Date  \n",
      "0  movie breath fresh air  fun romp europe favour...   Sep 7, 2019  \n",
      "1  amazing movie  many surprises movie  3d effect...  Jul 15, 2019  \n",
      "2  movie much better 1st installment  cgi good gy...  Sep 11, 2019  \n",
      "3  movie hands best mcu movie yet  beginning end ...   Jul 3, 2019  \n",
      "4  like folks absoultley adored movie ton action ...  Jul 13, 2019  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remote desktop\\AppData\\Local\\Temp\\ipykernel_47272\\3463117793.py:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Access the IMDB review page\n",
    "movie_id = \"tt6320628\" \n",
    "scrape_imdb_reviews(movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Function to get IMDb Top 250 Movie IDs\n",
    "def get_imdb_top_250():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in the background\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    url = \"https://www.imdb.com/chart/top/\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Allow time for the page to load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    movies = []\n",
    "    # for movie in soup.select(\"div.sc-ee514ad1-0.kYZRWL.cli-poster-container\"):\n",
    "    #     movie_title = movie.select_one(\"h3.ipc-title__text\")\n",
    "    #     movie_id = movie[\"href\"].split(\"/\")[2]  # Extracts the movie ID (e.g., tt0111161)\n",
    "    #     movies.append({\"movie_id\": movie_id, \"title\": movie_title})\n",
    "\n",
    "    for movie in soup.select(\"h3.ipc-title__text\"):  \n",
    "        movie_title = movie.text.strip()\n",
    "        movies.append({\"Movie Title\": movie_title})\n",
    "\n",
    "    return movies[:5]  # Return top 5 movies (change this if needed)\n",
    "\n",
    "top_movies = get_imdb_top_250()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(top_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "\n",
    "res = requests.get(\"https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm.I\")\n",
    "#print(res)\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "for card in soup.select('li.ipc-metadata-list-summary-item'):\n",
    "    data.append({\n",
    "        \"title\": card.select_one('h3.ipc-title__text').text.strip()\n",
    "        # \"year\": card.select_one('.titleColumn span').text,\n",
    "        # 'rating': card.select_one('td[class=\"ratingColumn imdbRating\"]').get_text(strip=True)\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "#df.to_csv('out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Idiots : 1187043\n",
      "3 Idiots : 3685624\n",
      "3 Idiots : 28238283\n",
      "3 Idiots w/GUNS : 0222441\n",
      "3 Idiots and a Wise Man : 21612358\n",
      "Three Idiots to a Team : 29720863\n",
      "Mugguru Monagallu : 15121916\n",
      "3 Idiots on Wheels : 6689378\n",
      "Scotch Mist - A Tale of Three English Idiots in Search of Britain's Northernmost Monsters : 31444403\n",
      "Kidnap in Rome : 1575673\n",
      "3 Idiots : 12049418\n",
      "3 Idiots : 33501685\n",
      "3 Idiot Heroes : 30247415\n",
      "Three Idiots : 16345748\n",
      "Three Idiots : 34207697\n",
      "The Idiots : 0154421\n",
      "3 Idiots Try Candy! : 8474256\n",
      "God and 3 Idiots : 25393152\n",
      "Confessions of Three Idiots : 21124554\n",
      "Idiots Are People Three! : 2179303\n"
     ]
    }
   ],
   "source": [
    "# importing the module \n",
    "import imdb \n",
    "   \n",
    "# creating instance of IMDb \n",
    "ia = imdb.IMDb() \n",
    "   \n",
    "# name  \n",
    "name = \"3 idiots\"\n",
    "   \n",
    "# searching the name  \n",
    "search = ia.search_movie(name) \n",
    "  \n",
    "  \n",
    "# loop for printing the name and id \n",
    "for i in range(len(search)): \n",
    "      \n",
    "    # getting the id \n",
    "    id = search[i].movieID \n",
    "      \n",
    "    # printing it \n",
    "    print(search[i]['title'] + \" : \" + id ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     filtered_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(word\u001b[38;5;241m.\u001b[39mtranslate(table) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_sentence\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 28\u001b[0m processed_titles \u001b[38;5;241m=\u001b[39m [preprocess_text(title) \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtitle_reviews\u001b[49m]\n\u001b[0;32m     29\u001b[0m processed_reviews \u001b[38;5;241m=\u001b[39m [preprocess_text(review) \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m content_reviews]\n\u001b[0;32m     31\u001b[0m reviews_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie Name\u001b[39m\u001b[38;5;124m\"\u001b[39m: movie_name,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview Title\u001b[39m\u001b[38;5;124m\"\u001b[39m: processed_titles,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview Content\u001b[39m\u001b[38;5;124m\"\u001b[39m: processed_reviews,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview Date\u001b[39m\u001b[38;5;124m\"\u001b[39m: review_date\n\u001b[0;32m     36\u001b[0m })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'title_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental arrows\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols & pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Symbols & pictographs extended-A\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols & pictographs extended-B\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# Remove stopwords，punctuations，html tags，emojis and transform into lowercase\n",
    "def preprocess_text(text):\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\",\", \" , \").replace(\".\", \" . \").replace(\"-\", \" - \").replace(\"/\", \" / \")\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    words = text.split()\n",
    "    filtered_sentence = \" \".join(word.translate(table) for word in words if word not in stop_words)\n",
    "    return filtered_sentence.strip()\n",
    "\n",
    "processed_titles = [preprocess_text(title) for title in title_reviews]\n",
    "processed_reviews = [preprocess_text(review) for review in content_reviews]\n",
    "\n",
    "reviews_df = pd.DataFrame({\n",
    "    \"Movie Name\": movie_name,\n",
    "    \"Review Title\": processed_titles,\n",
    "    \"Review Content\": processed_reviews,\n",
    "    \"Review Date\": review_date\n",
    "})\n",
    "\n",
    "csv_filename = f\"{movie_id}_Cleaned_Reviews.csv\"\n",
    "reviews_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(reviews_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
